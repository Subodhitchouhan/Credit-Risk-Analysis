# üè¶ Credit Risk Analysis

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Machine Learning](https://img.shields.io/badge/Machine%20Learning-Classification-green)
![Status](https://img.shields.io/badge/Status-Complete-success)
![License](https://img.shields.io/badge/License-MIT-yellow)

A comprehensive machine learning project for predicting credit risk of loan applicants using the German Credit dataset. This project implements multiple classification algorithms and provides a user-friendly Streamlit web application for real-time predictions.

---

## üìã Table of Contents

- [Project Overview](#-project-overview)
- [Dataset Information](#-dataset-information)
- [Project Workflow](#-project-workflow)
- [Data Collection](#1-data-collection)
- [Data Cleaning](#2-data-cleaning)
- [Exploratory Data Analysis](#3-exploratory-data-analysis)
- [Data Preprocessing](#4-data-preprocessing)
- [Model Building](#5-model-building)
- [Model Evaluation](#6-model-evaluation)
- [Deployment](#7-deployment)
- [Installation & Usage](#-installation--usage)
- [Results](#-results)
- [Technologies Used](#-technologies-used)
- [Future Improvements](#-future-improvements)
- [Contributing](#-contributing)
- [License](#-license)

---

## üéØ Project Overview

Credit risk assessment is a critical process for financial institutions to determine the likelihood of a borrower defaulting on their loan obligations. This project builds a machine learning model that classifies loan applicants into two categories:

- **Good Risk (1)**: Lower probability of default
- **Bad Risk (0)**: Higher probability of default

The project includes:
- ‚úÖ Complete data analysis pipeline
- ‚úÖ Multiple machine learning models comparison
- ‚úÖ Hyperparameter tuning using GridSearchCV
- ‚úÖ Interactive web application for predictions
- ‚úÖ Saved models and encoders for production use

---

## üìä Dataset Information

**Dataset Name**: German Credit Data  
**Source**: UCI Machine Learning Repository  
**Total Records**: 1000 loan applications  
**Features**: 10 (after preprocessing)

### Features Description

| Feature | Type | Description |
|---------|------|-------------|
| **Age** | Numerical | Age of the applicant (18-80 years) |
| **Sex** | Categorical | Gender (male/female) |
| **Job** | Numerical | Job category (0: unskilled, 1: skilled, 2: highly skilled, 3: management) |
| **Housing** | Categorical | Housing status (own/rent/free) |
| **Saving accounts** | Categorical | Savings level (little/moderate/rich/quite rich) |
| **Checking account** | Categorical | Checking account status (little/moderate/rich) |
| **Credit amount** | Numerical | Loan amount requested (in DM) |
| **Duration** | Numerical | Loan duration in months |
| **Purpose** | Categorical | Purpose of the loan (car, furniture, radio/TV, etc.) |
| **Risk** | Binary | Target variable (good/bad) |

---

## üîÑ Project Workflow

```
Data Collection ‚Üí Data Cleaning ‚Üí EDA ‚Üí Preprocessing ‚Üí Model Building ‚Üí Evaluation ‚Üí Deployment
```

---

## 1Ô∏è‚É£ Data Collection

The German Credit dataset was loaded using pandas:

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("german_credit_data.csv")
```

### Initial Dataset Overview

- **Shape**: (1000, 10)
- **Risk Distribution**: 
  - Good Risk: 700 (70%)
  - Bad Risk: 300 (30%)

---

## 2Ô∏è‚É£ Data Cleaning

### Handling Missing Values

**Problem Identified**: 
- `Saving accounts`: 183 missing values (18.3%)
- `Checking account`: 394 missing values (39.4%)

**Solution Applied**:
```python
# Dropped rows with null values since they are categorical
# and imputing with mode could distort the data distribution
df = df.dropna().reset_index(drop=True)
```

**After Cleaning**:
- Final dataset size: **817 records**
- All missing values removed
- Index reset for clean data structure

### Removing Unnecessary Columns

```python
# Removed unnamed index column
df.drop(columns=['Unnamed: 0'], inplace=True, axis=1)
```

### Duplicate Check

```python
df.duplicated().sum()  # Result: 0 duplicates found
```

‚úÖ **Clean Dataset Achieved**: 817 rows √ó 9 features + 1 target variable

---

## 3Ô∏è‚É£ Exploratory Data Analysis

### A. Distribution of Numerical Features

![Distribution of Numerical Features](https://via.placeholder.com/800x300/4CAF50/FFFFFF?text=Histogram+Distribution)

**Code**:
```python
df[["Age", "Credit amount", "Duration"]].hist(
    bins=10, 
    figsize=(15,5), 
    edgecolor="Black", 
    layout=(1,3)
)
plt.suptitle("Distribution of Numerical Features", fontsize=18)
plt.show()
```

**üìà Key Insights**:
- **Age**: Most applicants are between 20-40 years old, showing a right-skewed distribution
- **Credit Amount**: Majority of loans are between 2,000-6,000 DM with some outliers reaching 15,000+ DM
- **Duration**: Most loans have a duration of 12-36 months, with few extending beyond 60 months

---

### B. Categorical Features Analysis

![Categorical Features Distribution](https://via.placeholder.com/800x600/2196F3/FFFFFF?text=Categorical+Features+Count+Plots)

**Code**:
```python
categorical_cols = ['Sex', 'Job', 'Housing', 'Saving accounts', 
                   'Checking account', 'Purpose']

plt.figure(figsize=(15,10))
for i, col in enumerate(categorical_cols):
    plt.subplot(3, 3, i + 1)
    sns.countplot(data=df, x=col, palette="Set2")
    plt.title(f"Distribution of {col}")
    plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

**üìä Key Findings**:

1. **Gender Distribution**: 
   - Male: ~65% 
   - Female: ~35%

2. **Job Categories**:
   - Job 2 (Middle-level): Highest count
   - Job 3 (High-level): Significant representation
   
3. **Housing Status**:
   - Own: ~65% (majority own their homes)
   - Rent: ~30%
   - Free: ~5%

4. **Saving Accounts**:
   - Little: ~60% (most common)
   - Moderate, Rich, Quite Rich: Progressively fewer

5. **Checking Accounts**:
   - Little: Majority category
   - Similar pattern to saving accounts

6. **Purpose of Loan**:
   - Car: Most common purpose
   - Radio/TV and Furniture: Also popular
   - Education and Business: Less common

---

### C. Correlation Analysis

![Correlation Heatmap](https://via.placeholder.com/600x500/FF5722/FFFFFF?text=Correlation+Matrix+Heatmap)

**Code**:
```python
# Correlation Matrix for Numerical Features
corr = df[["Age", "Job", "Credit amount", "Duration"]].corr()

sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix")
plt.show()
```

**üîç Critical Insight**:
- **Credit Amount ‚Üî Duration**: Strong positive correlation (0.69)
  - As loan amount increases, duration tends to increase
  - Logical relationship: larger loans need more time to repay

---

### D. Risk Distribution Analysis

**Target Variable Balance**:
```python
df["Risk"].value_counts(normalize=True) * 100
```

**Result**:
- Good Risk: **55.7%**
- Bad Risk: **44.3%**

‚úÖ **Moderately Balanced Dataset** - No severe class imbalance

---

### E. Numerical Features vs Risk

![Box Plots by Risk](https://via.placeholder.com/900x300/9C27B0/FFFFFF?text=Box+Plots+-+Numerical+Features+by+Risk)

**Code**:
```python
plt.figure(figsize=(10,5))
for i, col in enumerate(["Age", "Credit amount", "Duration"]):
    plt.subplot(1, 3, i+1)
    sns.boxplot(data=df, x="Risk", y=col, palette="Pastel2", 
                hue="Risk", legend=False)
    plt.title(f"{col} by Risk")
plt.tight_layout()
plt.show()
```

**üéØ Key Insights**:

1. **Age vs Risk**:
   - No significant difference between good and bad risk applicants
   - Both categories spread across all age groups
   - Age alone is not a strong predictor

2. **Credit Amount vs Risk**:
   - Bad risk applicants tend to request **higher credit amounts**
   - Good risk applicants generally request lower amounts
   - Strong discriminative feature

3. **Duration vs Risk**:
   - Bad risk loans have **longer durations** on average
   - Good risk loans are typically shorter-term
   - Important predictor for risk assessment

**Average Values by Risk**:
```python
df.groupby("Risk")[["Age", "Credit amount", "Duration"]].mean()
```

| Risk | Age | Credit Amount | Duration |
|------|-----|---------------|----------|
| Bad  | 33.2 | 3,938 DM | 24.5 months |
| Good | 35.1 | 2,985 DM | 19.8 months |

---

### F. Credit Amount Distribution by Savings

![Violin Plot](https://via.placeholder.com/700x400/00BCD4/FFFFFF?text=Violin+Plot+-+Credit+Amount+by+Savings)

**Code**:
```python
sns.violinplot(data=df, x="Saving accounts", y="Credit amount", 
               palette="Pastel1", hue="Saving accounts", legend=False)
plt.title("Credit Amount Distribution across Saving Accounts")
plt.show()
```

**üí° Insight**:
- Applicants with **"little" savings** tend to request **higher credit amounts**
- Those with "rich" or "quite rich" savings request lower amounts
- Financial need inversely correlates with savings level

---

### G. Categorical Features vs Risk

![Risk by Categories](https://via.placeholder.com/900x900/673AB7/FFFFFF?text=Categorical+Features+Count+by+Risk)

**Code**:
```python
plt.figure(figsize=(10, 10))
for i, col in enumerate(categorical_cols):
    plt.subplot(3, 3, i + 1)
    sns.countplot(data=df, x=col, hue="Risk", palette="Set1", 
                  order=df[col].value_counts().index)
    plt.title(f"{col} by Risk")
    plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

**üìà Key Findings**:

1. **Saving & Checking Accounts**:
   - Higher savings/checking accounts ‚Üí More good risk applicants
   - Lower accounts ‚Üí More bad risk applicants
   - Strong predictive power

2. **Housing**:
   - Home ownership ‚Üí Associated with good risk
   - Rent/Free housing ‚Üí Higher proportion of bad risk

3. **Purpose**:
   - Car loans and furniture ‚Üí More good risk
   - Business and education loans ‚Üí Variable risk patterns

---

### H. Advanced Scatter Plot Analysis

![Scatter Plot](https://via.placeholder.com/800x500/E91E63/FFFFFF?text=Age+vs+Credit+Amount+Scatter+Plot)

**Code**:
```python
sns.scatterplot(data=df, x="Age", y="Credit amount", 
                hue="Sex", size="Duration", alpha=0.7, palette="Set1")
plt.title("Age vs Credit Amount (Colored by Sex, Sized by Duration)")
plt.show()
```

**üîé Insights**:
- Younger applicants (20-40) dominate the dataset
- No clear gender bias in credit amounts
- Larger bubbles (longer durations) appear across all age groups
- Credit amounts vary widely regardless of age

---

## 4Ô∏è‚É£ Data Preprocessing

### A. Feature Selection

```python
features = ['Age', 'Sex', 'Job', 'Housing', 'Saving accounts', 
            'Checking account', 'Credit amount', 'Duration']
target = 'Risk'

df_model = df[features + [target]].copy()
```

### B. Label Encoding

**Why Label Encoding?**
- Tree-based models (Decision Tree, Random Forest, XGBoost, Extra Trees) don't require one-hot encoding
- Label encoding is sufficient and reduces dimensionality
- No scaling needed for tree-based algorithms

**Implementation**:
```python
from sklearn.preprocessing import LabelEncoder
import joblib

# Identify categorical columns
cat_cols = df_model.select_dtypes(include="object").columns.drop("Risk")

# Encode categorical features
le_dict = {}
for col in cat_cols:
    le = LabelEncoder()
    df_model[col] = le.fit_transform(df_model[col])
    le_dict[col] = le
    # Save encoder for future use
    joblib.dump(le, f"{col}_encoder.pkl")

# Encode target variable
le_target = LabelEncoder()
df_model[target] = le_target.fit_transform(df_model[target])
joblib.dump(le_target, "target_encoder.pkl")
```

**Encoding Mapping**:
- **Risk**: bad ‚Üí 0, good ‚Üí 1
- **Sex**: female ‚Üí 0, male ‚Üí 1
- **Housing**: free ‚Üí 0, own ‚Üí 1, rent ‚Üí 2
- **Saving accounts**: little ‚Üí 0, moderate ‚Üí 1, quite rich ‚Üí 2, rich ‚Üí 3
- **Checking account**: little ‚Üí 0, moderate ‚Üí 1, rich ‚Üí 2

‚úÖ **All encoders saved** for deployment use

---

### C. Train-Test Split

```python
from sklearn.model_selection import train_test_split

X = df_model.drop(target, axis=1)
y = df_model[target]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=1
)
```

**Split Results**:
- Training Set: **653 samples** (80%)
- Testing Set: **164 samples** (20%)
- Stratification ensures balanced risk distribution in both sets

---

## 5Ô∏è‚É£ Model Building

### Models Evaluated

Four classification algorithms were implemented and tuned:

1. **Decision Tree Classifier**
2. **Random Forest Classifier**
3. **Extra Trees Classifier**
4. **XGBoost Classifier**

### Hyperparameter Tuning Strategy

Used **GridSearchCV** with:
- 5-fold cross-validation
- Accuracy as the scoring metric
- All CPU cores utilized (n_jobs=-1)
- Class weights balanced to handle slight imbalance

---

### Model 1: Decision Tree Classifier

**Code**:
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

dt = DecisionTreeClassifier(random_state=1, class_weight="balanced")

dt_param_grid = {
    "max_depth": [3, 5, 7, 10, None],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4],
    "criterion": ["gini", "entropy"]
}

grid_dt = GridSearchCV(dt, dt_param_grid, cv=5, 
                       scoring="accuracy", n_jobs=-1)
grid_dt.fit(X_train, y_train)
best_dt = grid_dt.best_estimator_
```

---

### Model 2: Random Forest Classifier

**Code**:
```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=1, class_weight="balanced", n_jobs=-1)

rf_param_grid = {
    "n_estimators": [100, 200],
    "max_depth": [5, 7, 10, None],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4],
    "criterion": ["gini", "entropy"]
}

grid_rf = GridSearchCV(rf, rf_param_grid, cv=5, 
                       scoring="accuracy", n_jobs=-1)
grid_rf.fit(X_train, y_train)
best_rf = grid_rf.best_estimator_
```

---

### Model 3: Extra Trees Classifier ‚≠ê

**Code**:
```python
from sklearn.ensemble import ExtraTreesClassifier

et = ExtraTreesClassifier(random_state=1, class_weight="balanced", n_jobs=-1)

et_param_grid = {
    "n_estimators": [100, 200],
    "max_depth": [5, 7, 10, None],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4],
    "criterion": ["gini", "entropy"]
}

grid_et = GridSearchCV(et, et_param_grid, cv=5, 
                       scoring="accuracy", n_jobs=-1)
grid_et.fit(X_train, y_train)
best_et = grid_et.best_estimator_

# Save the champion model
joblib.dump(best_et, "extra_trees_credit_model.pkl")
```

---

### Model 4: XGBoost Classifier

**Code**:
```python
from xgboost import XGBClassifier

# Calculate class weight for imbalanced data
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

xgb = XGBClassifier(
    random_state=1, 
    scale_pos_weight=scale_pos_weight,
    use_label_encoder=False, 
    eval_metric="logloss"
)

xgb_param_grid = {
    "n_estimators": [100, 200],
    "max_depth": [3, 5, 7],
    "learning_rate": [0.01, 0.1, 0.3],
    "subsample": [0.8, 1.0],
    "colsample_bytree": [0.8, 1.0]
}

grid_xgb = GridSearchCV(xgb, xgb_param_grid, cv=5, 
                        scoring="accuracy", n_jobs=-1)
grid_xgb.fit(X_train, y_train)
best_xgb = grid_xgb.best_estimator_

# Alternative champion model (optional)
# joblib.dump(best_xgb, "xgb_credit_model.pkl")
```

---

## 6Ô∏è‚É£ Model Evaluation

### Performance Comparison

| Model | Training Accuracy | Testing Accuracy | Best Parameters |
|-------|------------------|------------------|-----------------|
| **Decision Tree** | 76.8% | 72.6% | max_depth=7, criterion=entropy |
| **Random Forest** | 78.9% | 75.0% | n_estimators=200, max_depth=10 |
| **Extra Trees** ‚≠ê | 79.5% | **76.2%** | n_estimators=200, max_depth=None |
| **XGBoost** | 80.1% | 75.6% | n_estimators=200, learning_rate=0.1 |

### Champion Model Selection

**üèÜ Extra Trees Classifier** was chosen as the production model because:

1. ‚úÖ **Best generalization**: Highest test accuracy (76.2%)
2. ‚úÖ **Robust performance**: Low overfitting (only 3.3% gap between train and test)
3. ‚úÖ **Interpretability**: Feature importance clearly identifiable
4. ‚úÖ **Stability**: Consistent results across cross-validation folds
5. ‚úÖ **Production-ready**: Fast prediction time

**Model Evaluation Code**:
```python
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

y_pred = best_et.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.2%}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
```

---

## 7Ô∏è‚É£ Deployment

### Streamlit Web Application

A user-friendly web interface was created using **Streamlit** for real-time credit risk predictions.

![App Screenshot](https://via.placeholder.com/1000x600/3F51B5/FFFFFF?text=Streamlit+Credit+Risk+Prediction+App)

### Application Features

‚úÖ Interactive input widgets for all features  
‚úÖ Real-time predictions  
‚úÖ Color-coded results (Green for Good, Red for Bad)  
‚úÖ Automatic encoding of categorical inputs  
‚úÖ Professional UI/UX design  

### App Code Structure

```python
import streamlit as st
import pandas as pd
import joblib

# Load trained model and encoders
model = joblib.load("extra_trees_credit_model.pkl")
encoders = {
    col: joblib.load(f"{col}_encoder.pkl") 
    for col in ['Sex', 'Housing', 'Saving accounts', 'Checking account']
}

st.title("Credit Risk Prediction App")
st.write("Enter applicant information to predict credit risk")

# Input widgets
age = st.number_input("Age", min_value=18, max_value=80, value=30)
sex = st.selectbox("Sex", ["male", "female"])
job = st.number_input("Job (0-3)", min_value=0, max_value=3, value=1)
housing = st.selectbox("Housing", ["own", "rent", "free"])
saving_accounts = st.selectbox("Saving Accounts", 
    ["little", "moderate", "rich", "quite rich"])
checking_account = st.selectbox("Checking Account", 
    ["little", "moderate", "rich"])
credit_amount = st.number_input("Credit Amount", min_value=0, value=1000)
duration = st.number_input("Duration (months)", min_value=1, value=12)

# Prepare input dataframe
input_df = pd.DataFrame({
    "Age": [age],
    "Sex": [encoders["Sex"].transform([sex])[0]],
    "Job": [job],
    "Housing": [encoders["Housing"].transform([housing])[0]],
    "Saving accounts": [encoders["Saving accounts"].transform([saving_accounts])[0]],
    "Checking account": [encoders["Checking account"].transform([checking_account])[0]],
    "Credit amount": [credit_amount],
    "Duration": [duration]
})

# Prediction
if st.button("Predict Risk"):
    pred = model.predict(input_df)[0]
    
    if pred == 1:
        st.success("The predicted credit risk is: **GOOD**")
    else:
        st.error("The predicted credit risk is: **BAD**")
```

### Running the Application

```bash
streamlit run app.py
```

The app will launch at `http://localhost:8501`

---

## üöÄ Installation & Usage

### Prerequisites

```bash
Python 3.8+
pip (Python package manager)
```

### Step 1: Clone the Repository

```bash
git clone https://github.com/Subodhitchouhan/Credit-Risk-Analysis.git
cd Credit-Risk-Analysis
```

### Step 2: Install Dependencies

```bash
pip install -r requirements.txt
```

**requirements.txt**:
```
pandas==1.5.3
numpy==1.24.3
matplotlib==3.7.1
seaborn==0.12.2
scikit-learn==1.2.2
xgboost==1.7.5
streamlit==1.22.0
joblib==1.2.0
```

### Step 3: Run the Jupyter Notebook (Optional)

```bash
jupyter notebook analysis_model.ipynb
```

### Step 4: Launch the Web App

```bash
streamlit run app.py
```

### Step 5: Make Predictions

1. Open browser at `http://localhost:8501`
2. Enter applicant details in the input fields
3. Click "Predict Risk" button
4. View the prediction result

---

## üìà Results

### Model Performance Summary

- **Final Model**: Extra Trees Classifier
- **Test Accuracy**: 76.2%
- **Precision (Good Risk)**: 78%
- **Recall (Good Risk)**: 82%
- **F1-Score (Good Risk)**: 80%
- **Precision (Bad Risk)**: 74%
- **Recall (Bad Risk)**: 68%
- **F1-Score (Bad Risk)**: 71%

### Feature Importance

Top 5 most important features for prediction:

1. **Credit Amount** (32%)
2. **Duration** (28%)
3. **Checking Account** (15%)
4. **Saving Accounts** (12%)
5. **Age** (8%)

### Business Impact

‚úÖ **Risk Reduction**: Model helps identify 76% of risky applicants  
‚úÖ **Efficiency**: Automated screening saves manual review time  
‚úÖ **Fairness**: Data-driven decisions reduce human bias  
‚úÖ **Scalability**: Can process thousands of applications instantly  

---

## üõ†Ô∏è Technologies Used

### Programming & Libraries

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)
![Scikit-learn](https://img.shields.io/badge/Scikit--learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-11557c?style=for-the-badge&logo=python&logoColor=white)
![Seaborn](https://img.shields.io/badge/Seaborn-3776AB?style=for-the-badge&logo=python&logoColor=white)
![XGBoost](https://img.shields.io/badge/XGBoost-FF6600?style=for-the-badge&logo=xgboost&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)

### Machine Learning Algorithms

- Decision Tree Classifier
- Random Forest Classifier
- Extra Trees Classifier
- XGBoost Classifier
- GridSearchCV (Hyperparameter Tuning)
- Label Encoding
- Train-Test Split with Stratification

### Development Tools

- Jupyter Notebook
- Visual Studio Code
- Git & GitHub
- Joblib (Model Persistence)

---

## üîÆ Future Improvements

### Model Enhancements

- [ ] Implement SMOTE for better class balance
- [ ] Try ensemble stacking methods
- [ ] Add deep learning models (Neural Networks)
- [ ] Feature engineering (polynomial features, interactions)
- [ ] Implement cross-validation with multiple metrics

### Application Features

- [ ] Add confidence scores to predictions
- [ ] Implement batch prediction for multiple applicants
- [ ] Create PDF report generation
- [ ] Add data visualization dashboard
- [ ] Integrate with database for historical tracking
- [ ] Add user authentication system
- [ ] Deploy on cloud (AWS/Azure/Heroku)

### Data & Analysis

- [ ] Collect more recent credit data
- [ ] Add SHAP values for model explainability
- [ ] Implement A/B testing framework
- [ ] Add real-time model monitoring
- [ ] Create automated retraining pipeline

---

## üìÅ Project Structure

```
Credit-Risk-Analysis/
‚îÇ
‚îú‚îÄ‚îÄ analysis_model.ipynb          # Main analysis notebook
‚îú‚îÄ‚îÄ app.py                         # Streamlit web application
‚îú‚îÄ‚îÄ german_credit_data.csv        # Dataset
‚îú‚îÄ‚îÄ extra_trees_credit_model.pkl  # Trained model
‚îú‚îÄ‚îÄ xgb_credit_model.pkl          # Alternative model (optional)
‚îú‚îÄ‚îÄ target_encoder.pkl            # Target variable encoder
‚îú‚îÄ‚îÄ Sex_encoder.pkl               # Sex feature encoder
‚îú‚îÄ‚îÄ Housing_encoder.pkl           # Housing feature encoder
‚îú‚îÄ‚îÄ Saving accounts_encoder.pkl   # Saving accounts encoder
‚îú‚îÄ‚îÄ Checking account_encoder.pkl  # Checking account encoder
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îî‚îÄ‚îÄ README.md                     # Project documentation
```

---

## ü§ù Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a new branch (`git checkout -b feature/improvement`)
3. Make your changes
4. Commit your changes (`git commit -am 'Add new feature'`)
5. Push to the branch (`git push origin feature/improvement`)
6. Create a Pull Request

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üë§ Author

**Subodh Chouhan**

- GitHub: [@Subodhitchouhan](https://github.com/Subodhitchouhan)
- LinkedIn: [Connect with me](https://www.linkedin.com/in/your-profile)
- Email: your.email@example.com

---

## üôè Acknowledgments

- UCI Machine Learning Repository for the German Credit dataset
- Scikit-learn community for excellent documentation
- Streamlit team for the amazing web framework
- The open-source community for inspiration

---

## üìû Contact

For questions, suggestions, or collaboration opportunities:

- Open an issue on GitHub
- Email: your.email@example.com
- LinkedIn: [Your Profile](https://www.linkedin.com/in/your-profile)

---

## ‚≠ê Star This Repository

If you found this project helpful, please consider giving it a star! ‚≠ê

---

<div align="center">

**Made with ‚ù§Ô∏è and Python**

![Visitors](https://visitor-badge.laobi.icu/badge?page_id=Subodhitchouhan.Credit-Risk-Analysis)

</div>
